# ARLSTMForecaster

## Описание

**ARLSTMForecaster** — это рекуррентная нейронная сеть на базе LSTM с механизмом внимания (attention) для предсказания инвестиционных показателей по регионам России на несколько лет вперёд.  
Модель работает как авто-регрессивная (AR), используя прошлые значения временного ряда для прогнозирования будущих. Поддерживаются региональные эмбеддинги, проекция входа и несколько вариантов внимания.

---

## Архитектура

Основные компоненты модели:

- **Входные данные**:  
  Входной тензор `x` имеет размер `[batch_size, seq_len, n_features]`, где:
  - `seq_len` — длина временного окна (количество прошлых шагов для анализа),
  - `n_features` — количество признаков на каждый шаг.

- **Региональные эмбеддинги (опционально)**:  
  Если `region_emb_size > 0`, используется слой `nn.Embedding` для кодирования регионов в вектор фиксированной размерности.  
  Эти эмбеддинги конкатенируются с временными признаками перед подачей в LSTM-энкодер.

- **Проекция входа (опционально)**:  
  Слой `nn.Linear` может использоваться для уменьшения размерности входных данных перед подачей в энкодер.

- **Энкодер (LSTM)**:  
  Многослойный LSTM, который обрабатывает последовательность входных данных и формирует контекстный вектор `enc_ctx`.  
  Пуллинг контекста осуществляется конкатенацией последнего скрытого состояния и среднего по временной оси.

- **Декодер (LSTM)**:  
  Декодер получает предыдущие предсказания и скрытые состояния для генерации следующего шага.  
  Поддерживается возможность teacher forcing для обучения с известными целевыми значениями.

- **Механизм внимания**:  
  Используется dot-product или scaled dot-product внимание для взвешивания выходов энкодера при генерации каждого шага декодера.  
  Веса внимания могут подвергаться Dropout.

- **Выходной слой**:  
  Линейный слой объединяет выход декодера и контекст из энкодера для предсказания одного временного шага.

- **Инициализация весов**:  
  - Линейные слои: Xavier Uniform  
  - LSTM: weight_ih — Xavier, weight_hh — ортогональная (с fallback на Xavier), bias — нули, с единицами для forget-gate

---

## Использование

Пример создания модели и проверки количества параметров:

```python
import torch
from torch import nn

# Создание модели
test_model = ARLSTMForecaster(n_features=78, horizon=3)
print(test_model)

# Подсчет параметров
total_params = sum(p.numel() for p in test_model.parameters())
trainable_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)

print(f"Total parameters: {total_params}")
print(f"Trainable parameters: {trainable_params}")
```

---

## Основные гиперпараметры

| Параметр | Описание |
|----------|----------|
| `n_features` | Количество признаков на шаг |
| `horizon` | Количество шагов для предсказания |
| `encoder_hidden` | Размер скрытого состояния энкодера |
| `decoder_hidden` | Размер скрытого состояния декодера (если None, используется encoder_hidden) |
| `num_layers` | Количество слоев в энкодере и декодере |
| `dropout` | Доля отключаемых нейронов внутри LSTM |
| `input_proj_dim` | Размерность проекции входа (опционально) |
| `region_emb_size` | Размер регионального эмбеддинга |
| `attention_type` | Тип внимания: `"dot"` или `"scaled_dot"` |
| `attention_dropout` | Dropout для весов внимания |
| `input_noise_std` | Стандартное отклонение гауссовского шума на входе для регуляризации |

---

## Обучение

Обучение модели реализуется стандартным циклом PyTorch с возможностью настройки:

- Оптимизаторы: `Adam`, `AdamW`, `RMSProp`
- Темп обучения (`lr`) и L2-регуляризация (`weight_decay`)
- Dropout внутри LSTM и на выходе декодера
- Усечение градиентов (`grad_clip`)
- Тип функции потерь: `MSE`, `MAE`, `Huber`
- Teacher forcing с заданной вероятностью (`teacher_forcing_prob`)
- Поддержка LR scheduler: `StepLR` или `CosineAnnealingLR`

---

## Особенности

- LSTM с вниманием позволяет учитывать длинные временные зависимости.
- Поддержка авто-регрессивного режима для генерации последовательностей.
- Опциональные региональные эмбеддинги позволяют модели учитывать особенности каждого региона.
- Гибкая настройка декодера и энкодера.
- Совместимость с автоматизированным поиском гиперпараметров (Optuna).
- Поддержка регуляризации входного шума для улучшения обобщения.

---
