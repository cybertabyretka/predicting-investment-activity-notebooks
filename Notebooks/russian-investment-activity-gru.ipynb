{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e49c86-2d3b-43e9-b53d-09b5b329874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "import optuna\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e2bfa-7f83-4c82-a626-cbf7e6e4d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "path2data = \"kaggle/kaggle/russian_investment.csv\"\n",
    "df = pd.read_csv(path2data, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80488c4-544b-4d3b-9fb3-a13277a02673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "inv_col = 'capital_investments_million_rub'\n",
    "log_col = \"log_\" + inv_col\n",
    "inv_data = df[inv_col].astype(float)\n",
    "log_inv_data = df[log_col].astype(float)\n",
    "\n",
    "def plot_distribution(data, title, xlabel):\n",
    "    mean_val = data.mean()\n",
    "    median_val = data.median()\n",
    "    min_val = data.min()\n",
    "    max_val = data.max()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data, bins=50, color='#c7dcef', edgecolor='black', alpha=0.8)\n",
    "    plt.axvline(mean_val, color='blue', linestyle='-', linewidth=2, label=f'Среднее = {mean_val:.2f}')\n",
    "    plt.axvline(median_val, color='black', linestyle='-', linewidth=2, label=f'Медиана = {median_val:.2f}')\n",
    "    plt.axvline(min_val, color='red', linestyle='--', linewidth=2, label=f'Мин = {min_val:.2f}')\n",
    "    plt.axvline(max_val, color='red', linestyle='--', linewidth=2, label=f'Макс = {max_val:.2f}')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Количество')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title}.png')\n",
    "    plt.show()\n",
    "    \n",
    "plot_distribution(\n",
    "    df[inv_col].dropna().astype(float),\n",
    "    title='Распределение инвестиций в основной капитал',\n",
    "    xlabel='Инвестиции, млн руб'\n",
    ")\n",
    "plot_distribution(\n",
    "    df[log_col].dropna().astype(float),\n",
    "    title='Распределение логарифмированных инвестиций',\n",
    "    xlabel='log(Инвестиции + 1)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e82d31-7e1a-4e27-9a92-da9e3b228a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "SEQ_LEN = 4\n",
    "HORIZON = 3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "exclude_cols = [\n",
    "    \"year\", \n",
    "    \"region\", \n",
    "    \"district\", \n",
    "    'delta_target', \n",
    "    'delta_target_percent', \n",
    "    'log_target_next_year', \n",
    "    'target_next_year',  \n",
    "    'capital_investments_million_rub',\n",
    "    'gross_regional_product_lag_1',\n",
    "    'gross_regional_product_lag_2',\n",
    "    'population_lag_1',\n",
    "    'population_lag_2',\n",
    "    'unemployment_rate_15_72_years_percent_lag_1',\n",
    "    'unemployment_rate_15_72_years_percent_lag_2',\n",
    "    'average_salary_rub_lag_1',\n",
    "    'average_salary_rub_lag_2',\n",
    "    'retail_trade_turnover_million_rub_lag_1',\n",
    "    'retail_trade_turnover_million_rub_lag_2',\n",
    "    'total_money_income_million_rub_lag_1',\n",
    "    'total_money_income_million_rub_lag_2',\n",
    "    'internal_rnd_costs_million_rub_lag_1',\n",
    "    'internal_rnd_costs_million_rub_lag_2',\n",
    "    'accounts_payable_total_lag_1',\n",
    "    'accounts_payable_total_lag_2',\n",
    "    'number_of_crimes_registered_total'\n",
    "]\n",
    "\n",
    "TARGET_COL = log_col\n",
    "features = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "print(\"Число признаков:\", len(features))\n",
    "print(\"Признаки:\", features)\n",
    "print(\"TARGET_COL:\", TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951132b-d5a5-4e2c-997e-51c5f10baac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "df_work = df.copy()\n",
    "\n",
    "df_all = df_work.sort_values(['region_encoded', 'year']).reset_index(drop=True)\n",
    "last_year_in_data = df_all['year'].max()\n",
    "print(\"Последний год в данных:\", last_year_in_data)\n",
    "\n",
    "records = []\n",
    "regions = df_all['region_encoded'].unique()\n",
    "for region in regions:\n",
    "    grp = df_all[df_all['region_encoded'] == region].sort_values('year')\n",
    "    years = grp['year'].values\n",
    "    feat_mat = grp[features].values.astype(np.float32)\n",
    "    target_vec = grp[TARGET_COL].values.astype(np.float32)\n",
    "    n = len(grp)\n",
    "    max_i = n - (SEQ_LEN + HORIZON)\n",
    "    if max_i <= 0:\n",
    "        continue\n",
    "    for i in range(max_i + 1):\n",
    "        x_years = years[i:i+SEQ_LEN]\n",
    "        y_years = years[i+SEQ_LEN:i+SEQ_LEN+HORIZON]\n",
    "        X = feat_mat[i:i+SEQ_LEN]\n",
    "        Y = target_vec[i+SEQ_LEN:i+SEQ_LEN+HORIZON]\n",
    "        y_last = target_vec[i+SEQ_LEN-1]\n",
    "        records.append({\n",
    "            'region': int(region),\n",
    "            'start_year': int(x_years[0]),\n",
    "            'X': X,\n",
    "            'Y': Y,\n",
    "            'y_last': y_last,\n",
    "            'x_years': x_years,\n",
    "            'y_years': y_years\n",
    "        })\n",
    "\n",
    "print(\"Сгенерировано окон (records):\", len(records))\n",
    "\n",
    "train_records = [r for r in records if r['y_years'][-1] < last_year_in_data]\n",
    "val_records   = [r for r in records if r['y_years'][-1] == last_year_in_data]\n",
    "\n",
    "print(\"Train windows:\", len(train_records), \"Validation (holdout) windows:\", len(val_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1036d78-807d-4f15-a3da-e65a52239298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "all_X_train = np.concatenate([r['X'] for r in train_records], axis=0)\n",
    "scaler_x = StandardScaler().fit(all_X_train)\n",
    "\n",
    "all_Y_train = np.concatenate([r['Y'] for r in train_records], axis=0).reshape(-1, 1)\n",
    "scaler_y = StandardScaler().fit(all_Y_train)\n",
    "\n",
    "with open('gru_scaler_x.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_x, f)\n",
    "with open('gru_scaler_y.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_y, f)\n",
    "\n",
    "print(\"Скейлеры сохранены: gru_scaler_x.pkl, gru_scaler_y.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60380922-2248-4f9b-9b04-1780f3433d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "y_train_scaled = scaler_y.transform(\n",
    "    np.concatenate([r['Y'] for r in train_records], axis=0).reshape(-1, 1)\n",
    ").ravel()\n",
    "\n",
    "def plot_distribution_scaled(data, title, xlabel):\n",
    "    mean_val = data.mean()\n",
    "    median_val = np.median(data)\n",
    "    min_val = data.min()\n",
    "    max_val = data.max()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data, bins=50, color='#c7dcef', edgecolor='black', alpha=0.8)\n",
    "    plt.axvline(mean_val, color='blue', linestyle='-', linewidth=2,\n",
    "                label=f'Среднее = {mean_val:.2f}')\n",
    "    plt.axvline(median_val, color='black', linestyle='-', linewidth=2,\n",
    "                label=f'Медиана = {median_val:.2f}')\n",
    "    plt.axvline(min_val, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Мин = {min_val:.2f}')\n",
    "    plt.axvline(max_val, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Макс = {max_val:.2f}')\n",
    "\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Количество')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_distribution_scaled(\n",
    "    y_train_scaled,\n",
    "    title='Распределение таргетной переменной после StandardScaler',\n",
    "    xlabel='Scaled log(Инвестиции + 1)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd63b0-69e7-4ffc-a6d1-39fc503651dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "class SeqARDataset(Dataset):\n",
    "    def __init__(self, recs, scaler_x, scaler_y):\n",
    "        self.recs = recs\n",
    "        self.scaler_x = scaler_x\n",
    "        self.scaler_y = scaler_y\n",
    "        self.n_features = self.recs[0]['X'].shape[1] if len(self.recs)>0 else len(features)\n",
    "    def __len__(self):\n",
    "        return len(self.recs)\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.recs[idx]\n",
    "        X = r['X']\n",
    "        Xs = self.scaler_x.transform(X.reshape(-1, X.shape[1])).reshape(X.shape)\n",
    "        Y = r['Y'].reshape(-1, 1)\n",
    "        Ys = self.scaler_y.transform(Y).reshape(-1)\n",
    "        y_last = np.array([[r['y_last']]])\n",
    "        y_last_s = self.scaler_y.transform(y_last).reshape(())\n",
    "        return {\n",
    "            'X': torch.tensor(Xs, dtype=torch.float32),\n",
    "            'Y': torch.tensor(Ys, dtype=torch.float32),\n",
    "            'y_last': torch.tensor(y_last_s, dtype=torch.float32),\n",
    "            'region': int(r['region']),\n",
    "            'start_year': int(r['start_year'])\n",
    "        }\n",
    "\n",
    "train_ds = SeqARDataset(train_records, scaler_x, scaler_y)\n",
    "val_ds   = SeqARDataset(val_records, scaler_x, scaler_y)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False) if len(train_ds)>0 else None\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False) if len(val_ds)>0 else None\n",
    "\n",
    "print(\"Train batches:\", len(train_loader) if train_loader is not None else 0,\n",
    "      \"Val batches:\", len(val_loader) if val_loader is not None else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8007e3-307e-4a1a-859c-3cfe2c07b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "def torch_loss_mean(pred, target, loss_type, huber_delta=1.0):\n",
    "    if loss_type == \"mae\":\n",
    "        return torch.mean(torch.abs(pred - target))\n",
    "\n",
    "    elif loss_type == \"mse\":\n",
    "        return torch.mean((pred - target) ** 2)\n",
    "\n",
    "    elif loss_type == \"huber\":\n",
    "        err = pred - target\n",
    "        abs_err = torch.abs(err)\n",
    "        quadratic = 0.5 * err ** 2\n",
    "        linear = huber_delta * (abs_err - 0.5 * huber_delta)\n",
    "        loss_elem = torch.where(abs_err <= huber_delta, quadratic, linear)\n",
    "        return loss_elem.mean()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "\n",
    "def np_loss_per_horizon(y_true, y_pred, loss_type, huber_delta=1.0):\n",
    "    err = y_pred - y_true\n",
    "\n",
    "    if loss_type == \"mae\":\n",
    "        return np.mean(np.abs(err), axis=0)\n",
    "\n",
    "    elif loss_type == \"mse\":\n",
    "        return np.mean(err ** 2, axis=0)\n",
    "\n",
    "    elif loss_type == \"huber\":\n",
    "        abs_err = np.abs(err)\n",
    "        quadratic = 0.5 * err ** 2\n",
    "        linear = huber_delta * (abs_err - 0.5 * huber_delta)\n",
    "        loss_elem = np.where(abs_err <= huber_delta, quadratic, linear)\n",
    "        return np.mean(loss_elem, axis=0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type: {loss_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0afbae-47b4-434a-bd0f-ebcc683c0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "def validate_full(model, loader, scaler_y, loss_type, huber_delta=1.0, use_real_prev=False):\n",
    "    model.eval()\n",
    "    if loader is None:\n",
    "        return None, None\n",
    "\n",
    "    ys_scaled, yps_scaled = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            Y = batch[\"Y\"].to(device)\n",
    "            region = batch.get(\"region\", None)\n",
    "            if isinstance(region, torch.Tensor):\n",
    "                region = region.to(device)\n",
    "            pred = model(X, y=None, region=region, y_last=batch['y_last'].to(device) if use_real_prev else None, teacher_forcing=False)\n",
    "            \n",
    "            ys_scaled.append(Y.cpu().numpy())\n",
    "            yps_scaled.append(pred.cpu().numpy())\n",
    "\n",
    "    ys_scaled = np.concatenate(ys_scaled, axis=0)\n",
    "    yps_scaled = np.concatenate(yps_scaled, axis=0)\n",
    "\n",
    "    n, h = ys_scaled.shape\n",
    "    ys_log = scaler_y.inverse_transform(ys_scaled.reshape(-1, 1)).reshape(n, h)\n",
    "    yps_log = scaler_y.inverse_transform(yps_scaled.reshape(-1, 1)).reshape(n, h)\n",
    "\n",
    "    loss_per_h = np_loss_per_horizon(\n",
    "        ys_scaled, yps_scaled, loss_type, huber_delta\n",
    "    )\n",
    "    val_loss = float(np.mean(loss_per_h))\n",
    "\n",
    "    mae_log = np.mean(np.abs(ys_log - yps_log), axis=0)\n",
    "    mse_log = np.mean((ys_log - yps_log) ** 2, axis=0)\n",
    "    if huber_delta is None:\n",
    "        huber_delta = 1.0\n",
    "    huber_log = np_loss_per_horizon(\n",
    "        ys_log, yps_log, loss_type=\"huber\", huber_delta=huber_delta\n",
    "    )\n",
    "    r2_log = np.array(\n",
    "        [r2_score(ys_log[:, t], yps_log[:, t]) for t in range(h)]\n",
    "    )\n",
    "\n",
    "    ys_real = np.expm1(ys_log)\n",
    "    yps_real = np.expm1(yps_log)\n",
    "\n",
    "    mae_real = np.mean(np.abs(ys_real - yps_real), axis=0)\n",
    "    mse_real = np.mean((ys_real - yps_real) ** 2, axis=0)\n",
    "    rmse_real = np.sqrt(mse_real)\n",
    "    if huber_delta is None:\n",
    "        huber_delta = 1.0\n",
    "    huber_real = np_loss_per_horizon(\n",
    "        ys_real, yps_real, loss_type=\"huber\", huber_delta=huber_delta\n",
    "    )\n",
    "\n",
    "    eps = 1e-8\n",
    "    mape_real, smape_real = [], []\n",
    "\n",
    "    for t in range(h):\n",
    "        yt, yp = ys_real[:, t], yps_real[:, t]\n",
    "        nz = yt != 0\n",
    "\n",
    "        mape_real.append(\n",
    "            np.mean(np.abs((yt[nz] - yp[nz]) / yt[nz])) * 100.0\n",
    "            if nz.any()\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        smape_real.append(\n",
    "            np.mean(\n",
    "                np.abs(yp - yt) /\n",
    "                ((np.abs(yt) + np.abs(yp)) / 2 + eps)\n",
    "            ) * 100.0\n",
    "        )\n",
    "\n",
    "    metrics = {\n",
    "        \"LOSS_stand\": loss_per_h,\n",
    "\n",
    "        \"MAE_log\": mae_log,\n",
    "        \"MSE_log\": mse_log,\n",
    "        \"HUBER_log\": huber_log,\n",
    "        \"R2_log\": r2_log,\n",
    "\n",
    "        \"MAE_real\": mae_real,\n",
    "        \"MSE_real\": mse_real,\n",
    "        \"RMSE_real\": rmse_real,\n",
    "        \"HUBER_real\": huber_real,\n",
    "        \"MAPE_real\": np.array(mape_real),\n",
    "        \"SMAPE_real\": np.array(smape_real),\n",
    "    }\n",
    "\n",
    "    return val_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bd6e4-3b33-4fbf-83fa-4805de7929c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "def train_epoch_with_options(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    scaler_y,\n",
    "    loss_type,\n",
    "    huber_delta=1.0,\n",
    "    teacher_forcing_prob=0.0,\n",
    "    grad_clip=None,\n",
    "    device=device,\n",
    "    use_real_prev=False\n",
    "):\n",
    "    model.train()\n",
    "    total_loss, num_samples = 0.0, 0\n",
    "\n",
    "    scale_t = torch.tensor(scaler_y.scale_, dtype=torch.float32, device=device)\n",
    "    mean_t = torch.tensor(scaler_y.mean_, dtype=torch.float32, device=device)\n",
    "\n",
    "    for batch in loader:\n",
    "        X = batch[\"X\"].to(device)\n",
    "        Y = batch[\"Y\"].to(device)\n",
    "\n",
    "        region = batch.get(\"region\", None)\n",
    "        if isinstance(region, torch.Tensor):\n",
    "            region = region.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        use_tf = np.random.rand() < teacher_forcing_prob\n",
    "        pred = model(X, y=Y if use_tf else None, region=region, y_last=batch['y_last'].to(device) if use_real_prev else None, teacher_forcing=use_tf)\n",
    "\n",
    "        loss = torch_loss_mean(pred, Y, loss_type, huber_delta)\n",
    "        loss.backward()\n",
    "\n",
    "        if grad_clip and grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = X.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        num_samples += bs\n",
    "\n",
    "    return total_loss / max(1, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35205024-ad5d-40ce-abfb-b85ed00f3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "class ARGRUForecaster(nn.Module):\n",
    "    def __init__(self, n_features,\n",
    "                 encoder_hidden=128,\n",
    "                 decoder_hidden=None,\n",
    "                 num_layers=2,\n",
    "                 dropout=0.1,\n",
    "                 horizon=HORIZON,\n",
    "                 input_proj_dim=None,\n",
    "                 attention_type='dot',\n",
    "                 attention_dropout=0.0,\n",
    "                 region_emb_size=0,\n",
    "                 input_noise_std=0.0):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.n_features = n_features\n",
    "        self.encoder_hidden = encoder_hidden\n",
    "        self.decoder_hidden = decoder_hidden if decoder_hidden is not None else encoder_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_type = attention_type\n",
    "        self.attention_dropout_rate = attention_dropout\n",
    "        self.input_noise_std = input_noise_std\n",
    "        self.region_emb_size = int(region_emb_size)\n",
    "\n",
    "        if self.region_emb_size > 0:\n",
    "            self.region_emb = nn.Embedding(100, self.region_emb_size)\n",
    "            proj_in_features = n_features + self.region_emb_size\n",
    "        else:\n",
    "            self.region_emb = None\n",
    "            proj_in_features = n_features\n",
    "\n",
    "        if input_proj_dim is None:\n",
    "            self.input_proj = None\n",
    "            enc_input_size = proj_in_features\n",
    "        else:\n",
    "            self.input_proj = nn.Linear(proj_in_features, input_proj_dim)\n",
    "            enc_input_size = input_proj_dim\n",
    "\n",
    "        self.encoder = nn.GRU(input_size=enc_input_size,\n",
    "                              hidden_size=self.encoder_hidden,\n",
    "                              num_layers=self.num_layers,\n",
    "                              batch_first=True,\n",
    "                              dropout=dropout if num_layers > 1 else 0.0)\n",
    "\n",
    "        self.decoder = nn.GRU(input_size=1,\n",
    "                              hidden_size=self.decoder_hidden,\n",
    "                              num_layers=self.num_layers,\n",
    "                              batch_first=True,\n",
    "                              dropout=dropout if num_layers > 1 else 0.0)\n",
    "\n",
    "        if self.encoder_hidden != self.decoder_hidden:\n",
    "            self.h_proj = nn.Linear(self.encoder_hidden, self.decoder_hidden)\n",
    "            self.enc2dec = nn.Linear(self.encoder_hidden, self.decoder_hidden)\n",
    "        else:\n",
    "            self.h_proj = None\n",
    "            self.enc2dec = None\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(self.attention_dropout_rate) if self.attention_dropout_rate and self.attention_dropout_rate > 0 else None\n",
    "\n",
    "        self.out_proj = nn.Linear(self.decoder_hidden + self.encoder_hidden, 1)\n",
    "\n",
    "        self.start_proj = nn.Linear(self.encoder_hidden * 2, 1)\n",
    "\n",
    "        self.pool = lambda out: torch.cat([out[:, -1, :], out.mean(dim=1)], dim=1)\n",
    "\n",
    "        self.dec_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.zeros_(m.bias)\n",
    "\n",
    "        for module in (self.encoder, self.decoder):\n",
    "            for name, param in module.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    try:\n",
    "                        init.orthogonal_(param.data)\n",
    "                    except Exception:\n",
    "                        init.xavier_uniform_(param.data)\n",
    "                elif 'bias' in name:\n",
    "                    init.zeros_(param.data)\n",
    "\n",
    "    def _init_decoder_states(self, hn):\n",
    "        if self.h_proj is None:\n",
    "            return hn.contiguous()\n",
    "        else:\n",
    "            layers = []\n",
    "            for l in range(self.num_layers):\n",
    "                h_l = hn[l]\n",
    "                h_p = self.h_proj(h_l)\n",
    "                layers.append(h_p.unsqueeze(0))\n",
    "            dec_h = torch.cat(layers, dim=0).contiguous()\n",
    "            return dec_h\n",
    "\n",
    "    def forward(self, x, y=None, region=None, y_last=None, teacher_forcing=False):\n",
    "        B = x.size(0)\n",
    "        if self.region_emb is not None:\n",
    "            if region is None:\n",
    "                reg_emb = torch.zeros(B, self.region_emb_size, device=x.device)\n",
    "            else:\n",
    "                if isinstance(region, torch.Tensor):\n",
    "                    reg_idx = region.to(dtype=torch.long, device=x.device)\n",
    "                else:\n",
    "                    reg_idx = torch.tensor(region, dtype=torch.long, device=x.device)\n",
    "                reg_emb = self.region_emb(reg_idx)\n",
    "            reg_expand = reg_emb.unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "            x = torch.cat([x, reg_expand], dim=2)\n",
    "        if self.input_proj is not None:\n",
    "            x = self.input_proj(x)\n",
    "\n",
    "        if self.training and self.input_noise_std and self.input_noise_std > 0.0:\n",
    "            noise = torch.randn_like(x) * float(self.input_noise_std)\n",
    "            x = x + noise\n",
    "\n",
    "        enc_out, hn = self.encoder(x)\n",
    "        enc_ctx = self.pool(enc_out)\n",
    "\n",
    "        dec_h = self._init_decoder_states(hn)\n",
    "\n",
    "        if y_last is None:\n",
    "            prev = self.start_proj(enc_ctx).squeeze(1)\n",
    "        else:\n",
    "            prev = y_last\n",
    "\n",
    "        preds = []\n",
    "        for t in range(self.horizon):\n",
    "            inp = prev.unsqueeze(1).unsqueeze(2)\n",
    "            dec_out, dec_h = self.decoder(inp, dec_h)\n",
    "            dec_last = dec_h[-1]\n",
    "\n",
    "            if self.encoder_hidden != self.decoder_hidden and self.enc2dec is not None:\n",
    "                enc_proj = self.enc2dec(enc_out)\n",
    "                scores = torch.einsum('bsh,bh->bs', enc_proj, dec_last)\n",
    "            else:\n",
    "                scores = torch.einsum('bsh,bh->bs', enc_out, dec_last)\n",
    "\n",
    "            if self.attention_type == 'scaled_dot':\n",
    "                scores = scores / math.sqrt(max(1.0, float(self.encoder_hidden)))\n",
    "\n",
    "            attn_w = torch.softmax(scores, dim=1)\n",
    "            if self.attn_dropout is not None:\n",
    "                attn_w = self.attn_dropout(attn_w)\n",
    "\n",
    "            context = torch.einsum('bs,bsh->bh', attn_w, enc_out)\n",
    "\n",
    "            dec_last_drop = self.dec_dropout(dec_last)\n",
    "            out_inp = torch.cat([dec_last_drop, context], dim=1)\n",
    "            out_t = self.out_proj(out_inp).squeeze(1)\n",
    "\n",
    "            preds.append(out_t.unsqueeze(1))\n",
    "\n",
    "            if teacher_forcing and (y is not None):\n",
    "                prev = y[:, t] if t < y.size(1) else out_t\n",
    "            else:\n",
    "                prev = out_t\n",
    "\n",
    "        preds = torch.cat(preds, dim=1)\n",
    "        return preds\n",
    "\n",
    "test_model = ARGRUForecaster(n_features=78, horizon=3)\n",
    "print(test_model)\n",
    "total_params = sum(p.numel() for p in test_model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772429e-1de7-4dfb-8070-3cdf48ed6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "def create_model_from_cfg(cfg, state_dict=None, device=device):\n",
    "    model = ARGRUForecaster(\n",
    "        n_features=len(features),\n",
    "        encoder_hidden=cfg['encoder_hidden'],\n",
    "        decoder_hidden=cfg.get('decoder_hidden', None),\n",
    "        num_layers=cfg['num_layers'],\n",
    "        dropout=cfg['dropout'],\n",
    "        horizon=HORIZON,\n",
    "        input_proj_dim=cfg['input_proj_dim'],\n",
    "        attention_type=cfg['attention_type'],\n",
    "        attention_dropout=cfg.get('attention_dropout', 0.0),\n",
    "        region_emb_size=cfg.get('region_emb_size', 0),\n",
    "        input_noise_std=cfg.get('input_noise_std', 0.0)\n",
    "    ).to(device)\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f9408-d4f4-4cd8-9384-4919ac9fae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "GRID_EPOCHS = EPOCHS\n",
    "RANDOM_SEED = SEED\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-3, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\", \"rmsprop\"])\n",
    "\n",
    "    encoder_hidden = trial.suggest_categorical(\"encoder_hidden\", [8, 16, 32, 64, 128, 256])\n",
    "    decoder_hidden = trial.suggest_categorical(\"decoder_hidden\", [None, 8, 16, 32, 64, 128, 256])\n",
    "    region_emb_size = trial.suggest_categorical(\"region_emb_size\", [0, 8, 16, 32, 64])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "    input_proj_dim = trial.suggest_categorical(\"input_proj_dim\", [None, 8, 16, 32, 64, 128])\n",
    "    input_noise_std = trial.suggest_float(\"input_noise_std\", 0.0, 0.1)\n",
    "\n",
    "    attention_type = trial.suggest_categorical(\"attention_type\", [\"dot\", \"scaled_dot\"])\n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0.0, 0.4)\n",
    "\n",
    "    teacher_forcing_prob = trial.suggest_float(\"teacher_forcing_prob\", 0.0, 0.5)\n",
    "    grad_clip = trial.suggest_float(\"grad_clip\", 0.0, 5.0)\n",
    "\n",
    "    lr_scheduler = trial.suggest_categorical(\"lr_scheduler\", [\"none\", \"step\", \"cosine\"])\n",
    "    step_size = trial.suggest_int(\"step_size\", 10, max(10, GRID_EPOCHS//2))\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.1, 0.9)\n",
    "    loss_type = trial.suggest_categorical(\"loss_type\", [\"mae\", \"mse\", \"huber\"])\n",
    "    huber_delta = (trial.suggest_float(\"huber_delta\", 0.1, 5.0) if loss_type == \"huber\" else None)\n",
    "    use_real_prev = trial.suggest_categorical(\"use_real_prev\", [False, True])\n",
    "\n",
    "    cfg = {\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"encoder_hidden\": encoder_hidden,\n",
    "        \"decoder_hidden\": decoder_hidden,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout\": dropout,\n",
    "        \"input_proj_dim\": input_proj_dim,\n",
    "        'input_noise_std': input_noise_std,\n",
    "        \"attention_type\": attention_type,\n",
    "        \"attention_dropout\": attention_dropout,\n",
    "        \"teacher_forcing_prob\": teacher_forcing_prob,\n",
    "        \"grad_clip\": grad_clip,\n",
    "        \"lr_scheduler\": lr_scheduler,\n",
    "        \"step_size\": step_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"loss_type\": loss_type,\n",
    "        \"huber_delta\": huber_delta,\n",
    "        'use_real_prev': use_real_prev,\n",
    "        \"region_emb_size\": region_emb_size,\n",
    "    }\n",
    "\n",
    "    model = create_model_from_cfg(cfg)\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = None\n",
    "    if lr_scheduler == \"step\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=step_size, gamma=gamma\n",
    "        )\n",
    "    elif lr_scheduler == \"cosine\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=max(1, GRID_EPOCHS)\n",
    "        )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state_dict = None\n",
    "    best_cfg = None\n",
    "    best_epoch = None\n",
    "    epoch_history = []\n",
    "\n",
    "    patience = 15\n",
    "    min_delta = 1e-4\n",
    "    no_improve_steps = 0\n",
    "\n",
    "    for epoch in range(1, GRID_EPOCHS + 1):\n",
    "\n",
    "        train_loss = train_epoch_with_options(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            scaler_y,\n",
    "            loss_type=loss_type,\n",
    "            huber_delta=huber_delta,\n",
    "            teacher_forcing_prob=teacher_forcing_prob,\n",
    "            grad_clip=grad_clip,\n",
    "            use_real_prev=use_real_prev\n",
    "        )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        val_loss_full, metrics = validate_full(\n",
    "            model,\n",
    "            val_loader,\n",
    "            scaler_y,\n",
    "            loss_type=loss_type,\n",
    "            huber_delta=huber_delta,\n",
    "            use_real_prev=use_real_prev\n",
    "        )\n",
    "\n",
    "        epoch_history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss_full\": val_loss_full,\n",
    "            **{k: v.tolist() for k, v in metrics.items()}\n",
    "        })\n",
    "\n",
    "        if val_loss_full < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss_full\n",
    "            no_improve_steps = 0\n",
    "\n",
    "            best_state_dict = {\n",
    "                k: v.detach().cpu().clone()\n",
    "                for k, v in model.state_dict().items()\n",
    "            }\n",
    "            best_cfg = cfg.copy()\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            no_improve_steps += 1\n",
    "\n",
    "        trial.report(val_loss_full, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == GRID_EPOCHS:\n",
    "            print(\n",
    "                f\"[trial {trial.number}] \"\n",
    "                f\"epoch {epoch}/{GRID_EPOCHS} \"\n",
    "                f\"train={train_loss:.6f} \"\n",
    "                f\"val_full={val_loss_full:.6f} \"\n",
    "                f\"no_improve={no_improve_steps}/{patience}\"\n",
    "            )\n",
    "\n",
    "        if no_improve_steps >= patience:\n",
    "            print(\n",
    "                f\"[trial {trial.number}] \"\n",
    "                f\"Early stopping at epoch {epoch} \"\n",
    "                f\"(best_val={best_val_loss:.6f})\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    trial.set_user_attr(\"best_epoch\", best_epoch)\n",
    "    trial.set_user_attr(\"best_cfg\", best_cfg)\n",
    "    trial.set_user_attr(\"best_state_dict\", best_state_dict)\n",
    "    trial.set_user_attr(\"epoch_metrics\", epoch_history)\n",
    "\n",
    "    return float(best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6748568-29b6-4cbd-9f65-75e07730215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "study_name = f\"ar_gru_optuna_{int(time.time())}\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=study_name,\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_warmup_steps=max(1, GRID_EPOCHS // 10)\n",
    "    )\n",
    ")\n",
    "\n",
    "N_TRIALS = 200\n",
    "\n",
    "print(\"Starting Optuna study:\", study_name)\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "best = study.best_trial\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"cfg\": best.user_attrs[\"best_cfg\"],\n",
    "        \"state_dict\": best.user_attrs[\"best_state_dict\"],\n",
    "        \"epoch_metrics\": best.user_attrs[\"epoch_metrics\"],\n",
    "        \"best_val_loss\": best.value,\n",
    "        \"best_epoch\": best.user_attrs[\"best_epoch\"]\n",
    "\n",
    "    },\n",
    "    \"best_ar_gru.pt\"\n",
    ")\n",
    "print(f\"BEST EPOCH: {best.user_attrs['best_epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c785cb2-6d35-4d13-b00c-b8c34d8e1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16\n",
    "CKPT_PATH = \"best_ar_gru.pt\"\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "epoch_history = ckpt[\"epoch_metrics\"]\n",
    "best_epoch = ckpt.get(\"best_epoch\", None)\n",
    "\n",
    "print(f\"Loaded checkpoint: {CKPT_PATH}\")\n",
    "print(f\"Epochs saved: {len(epoch_history)}\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "\n",
    "print(\"\\nAvailable metric keys per epoch:\")\n",
    "for k in epoch_history[0].keys():\n",
    "    v = epoch_history[0][k]\n",
    "    if isinstance(v, list):\n",
    "        print(f\"  {k}: list(len={len(v)})\")\n",
    "    else:\n",
    "        print(f\"  {k}: scalar\")\n",
    "\n",
    "history = defaultdict(list)\n",
    "\n",
    "for row in epoch_history:\n",
    "    for k, v in row.items():\n",
    "        history[k].append(v)\n",
    "\n",
    "epochs = history[\"epoch\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(epochs, history[\"val_loss_full\"], label=\"val_loss\")\n",
    "\n",
    "if best_epoch is not None:\n",
    "    plt.axvline(best_epoch, color='red', linestyle='--', label=f\"best_epoch={best_epoch}\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"loss_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "def plot_per_horizon(metric_name, values, best_epoch=None, ylim_mode=\"auto\"):\n",
    "    arr = np.array(values)\n",
    "    H = arr.shape[1]\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    for h in range(H):\n",
    "        plt.plot(epochs, arr[:, h], label=f\"horizon {h+1}\")\n",
    "\n",
    "    if ylim_mode == \"auto\":\n",
    "        ymax = np.nanpercentile(arr, 95)\n",
    "        ymin = np.nanmin(arr)\n",
    "        plt.ylim(bottom=ymin * 0.95, top=ymax)\n",
    "    elif isinstance(ylim_mode, (int, float)):\n",
    "        plt.ylim(top=float(ylim_mode))\n",
    "\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            best_epoch,\n",
    "            color='red',\n",
    "            linestyle='--',\n",
    "            label=f\"best_epoch={best_epoch}\"\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(metric_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if ylim_mode is not None:\n",
    "        plt.savefig(f\"{metric_name} by Horizon ylim\")\n",
    "    else:\n",
    "        plt.savefig(f\"{metric_name} by Horizon\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nPlotting per-horizon metrics:\")\n",
    "\n",
    "for k, v in history.items():\n",
    "    if k in [\"epoch\", \"train_loss\", \"val_loss_full\"]:\n",
    "        continue\n",
    "\n",
    "    if isinstance(v[0], list):\n",
    "        try:\n",
    "            plot_per_horizon(\n",
    "                k, v,\n",
    "                best_epoch=best_epoch,\n",
    "            )\n",
    "            plot_per_horizon(\n",
    "                k, v,\n",
    "                best_epoch=best_epoch,\n",
    "                ylim_mode=None\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Skipped {k}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f2edf-387b-4c8b-8e91-3cc31ad0facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17\n",
    "ckpt = torch.load(\"best_ar_gru.pt\", map_location=device)\n",
    "model = create_model_from_cfg(\n",
    "    ckpt[\"cfg\"],\n",
    "    state_dict=ckpt[\"state_dict\"]\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e835f3-0e2e-469f-8393-007cd05e7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47151bdd-a159-479f-abbb-5d21bec5e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19\n",
    "model.eval()\n",
    "\n",
    "use_real_prev = ckpt['cfg']['use_real_prev']\n",
    "\n",
    "if val_loader is None:\n",
    "    print(\"No validation loader — ничего считать.\")\n",
    "else:\n",
    "    ys_scaled, yps_scaled = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            Y = batch[\"Y\"].to(device)\n",
    "            region = batch.get(\"region\", None)\n",
    "            if isinstance(region, torch.Tensor):\n",
    "                region = region.to(device)\n",
    "            pred = model(X, y=None, region=region, y_last=batch['y_last'].to(device) if use_real_prev else None, teacher_forcing=False)\n",
    "\n",
    "            ys_scaled.append(Y.cpu().numpy())\n",
    "            yps_scaled.append(pred.cpu().numpy())\n",
    "\n",
    "    ys_scaled = np.concatenate(ys_scaled, axis=0)\n",
    "    yps_scaled = np.concatenate(yps_scaled, axis=0)\n",
    "\n",
    "    n, h = ys_scaled.shape\n",
    "    ys_log = scaler_y.inverse_transform(ys_scaled.reshape(-1, 1)).reshape(n, h)\n",
    "    yps_log = scaler_y.inverse_transform(yps_scaled.reshape(-1, 1)).reshape(n, h)\n",
    "\n",
    "    mae_log = np_loss_per_horizon(ys_log, yps_log, loss_type=\"mae\")\n",
    "    mse_log = np_loss_per_horizon(ys_log, yps_log, loss_type=\"mse\")\n",
    "    if ckpt['cfg']['huber_delta'] is None:\n",
    "        huber_delta = 1.0\n",
    "    else:\n",
    "        huber_delta = ckpt['cfg']['huber_delta']\n",
    "    huber_log = np_loss_per_horizon(\n",
    "        ys_log, yps_log, loss_type=\"huber\", huber_delta=huber_delta\n",
    "    )\n",
    "    r2_log = np.array([\n",
    "        r2_score(ys_log[:, t], yps_log[:, t]) for t in range(h)\n",
    "    ])\n",
    "\n",
    "    ys_real = np.expm1(ys_log)\n",
    "    yps_real = np.expm1(yps_log)\n",
    "\n",
    "    mae_real = np_loss_per_horizon(ys_real, yps_real, loss_type=\"mae\")\n",
    "    mse_real = np_loss_per_horizon(ys_real, yps_real, loss_type=\"mse\")\n",
    "    rmse_real = np.sqrt(mse_real)\n",
    "    huber_real = np_loss_per_horizon(\n",
    "        ys_real, yps_real, loss_type=\"huber\", huber_delta=huber_delta\n",
    "    )\n",
    "\n",
    "    eps = 1e-8\n",
    "    mape_real, smape_real = [], []\n",
    "\n",
    "    for t in range(h):\n",
    "        yt, yp = ys_real[:, t], yps_real[:, t]\n",
    "        nz = yt != 0\n",
    "\n",
    "        mape_real.append(\n",
    "            np.mean(np.abs((yt[nz] - yp[nz]) / yt[nz])) * 100.0\n",
    "            if nz.any()\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        smape_real.append(\n",
    "            np.mean(\n",
    "                np.abs(yp - yt) /\n",
    "                ((np.abs(yt) + np.abs(yp)) / 2 + eps)\n",
    "            ) * 100.0\n",
    "        )\n",
    "\n",
    "    mape_real = np.array(mape_real)\n",
    "    smape_real = np.array(smape_real)\n",
    "\n",
    "    for t in range(h):\n",
    "        print(\n",
    "            f\"Год t+{t+1}: \"\n",
    "            f\"MAE_log={mae_log[t]:.6f}, \"\n",
    "            f\"MSE_log={mse_log[t]:.6f}, \"\n",
    "            f\"R2_log={r2_log[t]:.6f}, \"\n",
    "            f\"HUBER_log={huber_log[t]:.6f} | \"\n",
    "            f\"MAE_real={mae_real[t]:.2f}, \"\n",
    "            f\"MSE_real={mse_real[t]:.2f}, \"\n",
    "            f\"RMSE_real={rmse_real[t]:.2f}, \"\n",
    "            f\"HUBER_real={huber_real[t]:.2f}, \"\n",
    "            f\"MAPE_real={mape_real[t]:.2f}%, \"\n",
    "            f\"SMAPE_real={smape_real[t]:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c60dd-77e3-4bd7-a9c4-3d8d7690de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20\n",
    "idx_to_name_path = \"idx_to_name.pkl\"\n",
    "idx_to_name = {}\n",
    "if os.path.exists(idx_to_name_path):\n",
    "    with open(idx_to_name_path, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "    idx_to_name = d.get(\"region\", {})\n",
    "    print(f\"Loaded region mapping from {idx_to_name_path}, entries={len(idx_to_name)}\")\n",
    "else:\n",
    "    print(f\"Warning: {idx_to_name_path} not found — регионы будут показаны как индексы.\")\n",
    "\n",
    "need_rebuild = not all(var in globals() for var in [\"ys_all_scaled\", \"yps_all_scaled\", \"regions_all\"])\n",
    "if need_rebuild:\n",
    "    if 'val_loader' not in globals() or val_loader is None:\n",
    "        raise RuntimeError(\"val_loader отсутствует — выполните этап валидации или обеспечьте val_loader.\")\n",
    "    print(\"Пересобираю ys_all_scaled, yps_all_scaled, regions_all из val_loader ...\")\n",
    "\n",
    "    ys_list, yps_list, regions_list = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X = batch['X'].to(device)\n",
    "            Y = batch['Y'].to(device)\n",
    "            region = batch['region']\n",
    "            pred = model(X, y=None, region=region, y_last=batch['y_last'].to(device) if use_real_prev else None, teacher_forcing=False)\n",
    "            ys_list.append(Y.cpu().numpy())\n",
    "            yps_list.append(pred.cpu().numpy())\n",
    "            if isinstance(region, torch.Tensor):\n",
    "                regions_list.append(region.cpu().numpy())\n",
    "            else:\n",
    "                regions_list.append(np.array(region))\n",
    "\n",
    "    ys_all_scaled = np.concatenate(ys_list, axis=0)\n",
    "    yps_all_scaled = np.concatenate(yps_list, axis=0)\n",
    "    regions_all = np.concatenate(regions_list, axis=0)\n",
    "    print(f\"Собрано: окна={ys_all_scaled.shape[0]}, horizon={ys_all_scaled.shape[1]}\")\n",
    "\n",
    "n, h = ys_all_scaled.shape\n",
    "assert yps_all_scaled.shape == (n, h)\n",
    "assert regions_all.shape[0] == n\n",
    "\n",
    "ys_all_log = scaler_y.inverse_transform(ys_all_scaled.reshape(-1,1)).reshape(n,h)\n",
    "yps_all_log = scaler_y.inverse_transform(yps_all_scaled.reshape(-1,1)).reshape(n,h)\n",
    "\n",
    "ys_all_real = np.expm1(ys_all_log)\n",
    "yps_all_real = np.expm1(yps_all_log)\n",
    "\n",
    "region_metrics = {}\n",
    "unique_regions = np.unique(regions_all)\n",
    "eps = 1e-8\n",
    "\n",
    "for reg in unique_regions:\n",
    "    mask = regions_all == reg\n",
    "    cnt = int(mask.sum())\n",
    "    if cnt == 0:\n",
    "        continue\n",
    "\n",
    "    ys_r_log, yps_r_log = ys_all_log[mask], yps_all_log[mask]\n",
    "    ys_r_real, yps_r_real = ys_all_real[mask], yps_all_real[mask]\n",
    "\n",
    "    mae_log_r = np_loss_per_horizon(ys_r_log, yps_r_log, loss_type=\"mae\")\n",
    "    mse_log_r = np_loss_per_horizon(ys_r_log, yps_r_log, loss_type=\"mse\")\n",
    "    if ckpt['cfg']['huber_delta'] is None:\n",
    "        huber_delta = 1.0\n",
    "    else:\n",
    "        huber_delta = ckpt['cfg']['huber_delta']\n",
    "    huber_log_r = np_loss_per_horizon(ys_r_log, yps_r_log, loss_type=\"huber\", huber_delta=huber_delta)\n",
    "\n",
    "    mae_real_r = np_loss_per_horizon(ys_r_real, yps_r_real, loss_type=\"mae\")\n",
    "    mse_real_r = np_loss_per_horizon(ys_r_real, yps_r_real, loss_type=\"mse\")\n",
    "    rmse_real_r = np.sqrt(mse_real_r)\n",
    "    huber_real_r = np_loss_per_horizon(ys_r_real, yps_r_real, loss_type=\"huber\", huber_delta=huber_delta)\n",
    "\n",
    "    mape_real_r, smape_real_r = [], []\n",
    "    for t in range(h):\n",
    "        yt, yp = ys_r_real[:,t], yps_r_real[:,t]\n",
    "        nz = yt != 0\n",
    "        mape_real_r.append(np.mean(np.abs((yt[nz]-yp[nz])/yt[nz]))*100.0 if nz.any() else np.nan)\n",
    "        smape_real_r.append(np.mean(np.abs(yp-yt)/((np.abs(yt)+np.abs(yp))/2 + eps))*100.0)\n",
    "    mape_real_r, smape_real_r = np.array(mape_real_r), np.array(smape_real_r)\n",
    "\n",
    "    region_metrics[int(reg)] = {\n",
    "        'count': cnt,\n",
    "        'MAE_log': mae_log_r,\n",
    "        'MSE_log': mse_log_r,\n",
    "        'HUBER_log': huber_log_r,\n",
    "        'MAE_real': mae_real_r,\n",
    "        'MSE_real': mse_real_r,\n",
    "        'RMSE_real': rmse_real_r,\n",
    "        'HUBER_real': huber_real_r,\n",
    "        'MAPE_real': mape_real_r,\n",
    "        'SMAPE_real': smape_real_r\n",
    "    }\n",
    "\n",
    "print(\"=== Метрики по регионам (название / индекс) ===\")\n",
    "for reg in sorted(region_metrics.keys()):\n",
    "    name = idx_to_name.get(reg, None)\n",
    "    title = f\"{name} (idx={reg})\" if name else f\"idx={reg}\"\n",
    "    print(f\"Регион: {title}  — окна: {region_metrics[reg]['count']}\")\n",
    "    for t in range(h):\n",
    "        print(\n",
    "            f\"  t+{t+1}: \"\n",
    "            f\"MAE_log={region_metrics[reg]['MAE_log'][t]:.6f}, \"\n",
    "            f\"MSE_log={region_metrics[reg]['MSE_log'][t]:.6f}, \"\n",
    "            f\"HUBER_log={region_metrics[reg]['HUBER_log'][t]:.6f} | \"\n",
    "            f\"MAE_real={region_metrics[reg]['MAE_real'][t]:.2f}, \"\n",
    "            f\"MSE_real={region_metrics[reg]['MSE_real'][t]:.2f}, \"\n",
    "            f\"RMSE_real={region_metrics[reg]['RMSE_real'][t]:.2f}, \"\n",
    "            f\"HUBER_real={region_metrics[reg]['HUBER_real'][t]:.2f}, \"\n",
    "            f\"MAPE_real={region_metrics[reg]['MAPE_real'][t]:.2f}%, \"\n",
    "            f\"SMAPE_real={region_metrics[reg]['SMAPE_real'][t]:.2f}%\"\n",
    "        )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61982c9-effc-4c95-9e95-197f97b67ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 21\n",
    "TOP_K = 8\n",
    "N_REPEATS = 10\n",
    "VERBOSE = True\n",
    "\n",
    "if 'val_loader' not in globals() or val_loader is None:\n",
    "    raise RuntimeError(\"val_loader отсутствует или None — подготовьте валидатор (val_loader) перед запуском этой ячейки.\")\n",
    "\n",
    "if 'model' not in globals():\n",
    "    raise RuntimeError(\"model не найден в текущем пространстве имён.\")\n",
    "\n",
    "if 'scaler_y' not in globals():\n",
    "    raise RuntimeError(\"scaler_y не найден (нужен для inverse_transform).\")\n",
    "\n",
    "model.eval()\n",
    "ys_list, yps_list = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        X = batch['X'].to(device)\n",
    "        Y = batch['Y'].to(device)\n",
    "        region = batch.get(\"region\", None)\n",
    "        if isinstance(region, torch.Tensor):\n",
    "            region = region.to(device)\n",
    "        pred = model(X, y=None, region=region, y_last=batch['y_last'].to(device) if use_real_prev else None, teacher_forcing=False)\n",
    "        ys_list.append(Y.cpu().numpy())\n",
    "        yps_list.append(pred.cpu().numpy())\n",
    "\n",
    "ys_all_scaled = np.concatenate(ys_list, axis=0)\n",
    "yps_all_scaled = np.concatenate(yps_list, axis=0)\n",
    "\n",
    "n, h = ys_all_scaled.shape\n",
    "ys_all_log = scaler_y.inverse_transform(ys_all_scaled.reshape(-1,1)).reshape(n,h)\n",
    "yps_all_log = scaler_y.inverse_transform(yps_all_scaled.reshape(-1,1)).reshape(n,h)\n",
    "\n",
    "ys_all_real = np.expm1(ys_all_log)\n",
    "yps_all_real = np.expm1(yps_all_log)\n",
    "\n",
    "baseline_mae = np.mean(np.abs(ys_all_real - yps_all_real))\n",
    "if VERBOSE:\n",
    "    print(f\"Baseline MAE (real, averaged over все окна и горизонты): {baseline_mae:.6f}\")\n",
    "\n",
    "first_batch = None\n",
    "for batch in val_loader:\n",
    "    first_batch = batch\n",
    "    break\n",
    "\n",
    "X_sample = first_batch['X']\n",
    "X_shape = tuple(X_sample.shape)\n",
    "if len(X_shape) == 3:\n",
    "    _, seq_len, n_features = X_shape\n",
    "elif len(X_shape) == 2:\n",
    "    _, n_features = X_shape\n",
    "    seq_len = 1\n",
    "else:\n",
    "    raise RuntimeError(f\"Неожиданная форма batch['X']: {X_shape}\")\n",
    "\n",
    "if 'features' not in globals():\n",
    "    raise RuntimeError(\"Переменная 'features' не найдена — в ней должны быть имена признаков.\")\n",
    "\n",
    "feat_names = list(features)\n",
    "\n",
    "if len(feat_names) != n_features:\n",
    "    raise RuntimeError(\n",
    "        f\"Размер features ({len(feat_names)}) != количеству признаков в X ({n_features}). \"\n",
    "        \"Проверьте порядок или длину features.\"\n",
    "    )\n",
    "\n",
    "def mae_with_permuted_feature(feature_idx):\n",
    "    ys_parts, yps_parts = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            Xb = batch['X'].to(device)\n",
    "            Yb = batch['Y'].to(device)\n",
    "            Xp = Xb.clone()\n",
    "            B = Xp.size(0)\n",
    "            if B <= 1:\n",
    "                if seq_len > 1:\n",
    "                    perm = torch.randperm(seq_len).to(device)\n",
    "                    Xp[:, :, feature_idx] = Xp[:, perm, feature_idx]\n",
    "            else:\n",
    "                perm_idx = torch.randperm(B).to(device)\n",
    "                Xp[:, :, feature_idx] = Xp[perm_idx, :, feature_idx]\n",
    "            region = batch.get(\"region\", None)\n",
    "            if isinstance(region, torch.Tensor):\n",
    "                region = region.to(device)\n",
    "            predp = model(Xp, y=None, region=region, y_last=batch['y_last'].to(device) if use_real_prev else None, teacher_forcing=False)\n",
    "            ys_parts.append(Yb.cpu().numpy())\n",
    "            yps_parts.append(predp.cpu().numpy())\n",
    "\n",
    "    ys_scaled = np.concatenate(ys_parts, axis=0)\n",
    "    yps_scaled = np.concatenate(yps_parts, axis=0)\n",
    "    ys_log = scaler_y.inverse_transform(ys_scaled.reshape(-1,1)).reshape(-1, h)\n",
    "    yps_log = scaler_y.inverse_transform(yps_scaled.reshape(-1,1)).reshape(-1, h)\n",
    "    ys_real = np.expm1(ys_log)\n",
    "    yps_real = np.expm1(yps_log)\n",
    "    mae = np.mean(np.abs(ys_real - yps_real))\n",
    "    return mae\n",
    "\n",
    "importances = np.zeros(n_features, dtype=float)\n",
    "for i in range(n_features):\n",
    "    vals = []\n",
    "    for _ in range(N_REPEATS):\n",
    "        mae_perm = mae_with_permuted_feature(i)\n",
    "        vals.append(mae_perm)\n",
    "    mae_perm_avg = float(np.mean(vals))\n",
    "    importance = mae_perm_avg - baseline_mae\n",
    "    importances[i] = max(0.0, importance)\n",
    "\n",
    "total_imp = importances.sum()\n",
    "if total_imp == 0:\n",
    "    percent = np.zeros_like(importances)\n",
    "else:\n",
    "    percent = importances / total_imp * 100.0\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'feature': feat_names,\n",
    "    'importance': importances,\n",
    "    'percent': percent\n",
    "}).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "K = int(TOP_K)\n",
    "top_df = df.head(K).copy()\n",
    "others_sum = df['importance'].iloc[K:].sum()\n",
    "others_pct = df['percent'].iloc[K:].sum()\n",
    "if others_sum > 0:\n",
    "    top_df = pd.concat([\n",
    "        top_df,\n",
    "        pd.DataFrame([{'feature': 'Другие', 'importance': others_sum, 'percent': others_pct}])\n",
    "    ], ignore_index=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "labels = top_df['feature'].tolist()\n",
    "sizes = top_df['importance'].tolist()\n",
    "\n",
    "plt.pie(sizes, \n",
    "        labels=labels, \n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90, \n",
    "        counterclock=False,\n",
    "        textprops={'fontsize': 16})\n",
    "\n",
    "plt.title(f\"Вклад признаков в MAE (top {K} + Другие)\", fontsize=20)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"Вклад_признаков_в_MAE.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
